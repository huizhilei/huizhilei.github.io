<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="word2vec论文, 寂寞沙洲冷">
    <meta name="description" content="Efficient Estimation of Word Representations in  Vector Space(本文从经验上理解论文，不注重数学推导)语言模型： 首先回顾一下自然语言处理中的一个基本问题：如何计算一段文本序列在某">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>word2vec论文 | 寂寞沙洲冷</title>
    <link rel="icon" type="image/png" href="/favicon.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 5.4.0"></head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">寂寞沙洲冷</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">寂寞沙洲冷</div>
        <div class="logo-desc">
            
            Never really desperate, only the lost of the soul.
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/huizhilei" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/huizhilei" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/17.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">word2vec论文</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/Efficient-Estimation-of-Word-Representations-in-Vector-Space/">
                                <span class="chip bg-color">Efficient Estimation of Word Representations in Vector Space</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2021-11-06
                </div>
                

                

                

                

                
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Efficient-Estimation-of-Word-Representations-in-Vector-Space"><a href="#Efficient-Estimation-of-Word-Representations-in-Vector-Space" class="headerlink" title="Efficient Estimation of Word Representations in  Vector Space"></a><strong>Efficient Estimation of Word Representations in  Vector Space</strong></h1><h2 id="本文从经验上理解论文，不注重数学推导"><a href="#本文从经验上理解论文，不注重数学推导" class="headerlink" title="(本文从经验上理解论文，不注重数学推导)"></a><strong>(本文从经验上理解论文，不注重数学推导)</strong></h2><h1 id="语言模型："><a href="#语言模型：" class="headerlink" title="语言模型："></a>语言模型：</h1><p> 首先回顾一下自然语言处理中的一个基本问题：<strong>如何计算一段文本序列在某种语言下出现的概率？</strong> </p>
<p> 对于一段文本序列：<br>$$<br>S=w_1, w_2, … , w_T<br>$$<br> 语言模型（language model）的目标是估计序列的联合概率，基于链式法则可以转化为条件概率分布的乘积，如下所示：<br>$$<br>P(S)=P(w_1, w_2, …, w_T)=\prod_{t=1}^Tp(w_t|w_1, w_2, …, w_{t-1})<br>$$<br> <strong>即将序列的联合概率转化为一系列条件概率的乘积。问题变成了如何去预测这些给定previous words下的条件概率：</strong><br>$$<br>p(w_t|w_1,w_2,…,w_{t-1})<br>$$<br> 由于其巨大的参数空间，这样一个原始的模型在实际中并没有什么用。  我们可以基于马尔科夫假设来做简化。 </p>
<p><strong>什么是马尔科夫假设？</strong></p>
<p>马尔科夫假设是指，每个词出现的概率只跟它前面的少数几个词有关。引入了马尔科夫假设的语言模型，也可以叫做马尔科夫模型。 也就是说，应用了这个假设表明了当前这个词仅仅跟前面几个有限的词相关，因此也就不必追溯到最开始的那个词，这样便可以大幅缩减上述算式的长度。 </p>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1><h1 id="简化版本：N-gram模型："><a href="#简化版本：N-gram模型：" class="headerlink" title="简化版本：N-gram模型："></a><strong>简化版本：N-gram模型：</strong></h1><p>$$<br>p(w_t|w_1, w_2, …, w_{t-1}) \approx p(w_t|w_{t-n+1}, …, w_{t-1})<br>$$</p>
<p> <strong>求解Ngram模型的参数——等价于去统计每个Ngram的条件词频 。</strong></p>
<p>————————————————————————————————</p>
<p>关于ngram小思考（不知道对不对），N-gram总共需要存储V^N个参数（概率值），其中V是词汇表中词元个数。</p>
<p>————————————————————————————————</p>
<p><strong>不过，Ngram模型仍有其局限性。首先，由于参数空间的爆炸式增长，它无法处理更长程的context（N&gt;3）。其次，它没有考虑词与词之间内在的联系性。</strong> </p>
<p><strong>这是因为，Ngram本质上是将词当做一个个孤立的原子单元（atomic unit）去处理的。这种处理方式对应到数学上的形式是一个个离散的one-hot向量 。</strong></p>
<p>————————————————————————————————</p>
<p>关于Ngram算法atomic unit的小思考：Ngram算法关于每个词元的处理，并没有考虑其语义信息，只是简单粗暴地将所有词元以N为单位进行排列组合，计算各个gram后任意词元出现的频率，（语料库确定了，Ngram模型其实就确定了）其实，其本质上，是所谓的atomic unit。</p>
<p>————————————————————————————————</p>
<p> 于是，人们就自然而然地想到，能否用一个<strong>连续的稠密向量去刻画一个word的特征</strong>呢？ 如果可以， 我们就可以直接刻画词与词之间的相似度 。<strong>word2vec 将每个词映射到一个固定长度的向量，这些向量能更好地表达不同词之间的相似性和类比关系。</strong> </p>
<h1 id="Neural-Network-Language-Model（NNLM）"><a href="#Neural-Network-Language-Model（NNLM）" class="headerlink" title="Neural Network Language Model（NNLM）"></a>Neural Network Language Model（NNLM）</h1><p>参考：<a target="_blank" rel="noopener" href="https://www.cnblogs.com/guoyaohua/p/9240336.html">https://www.cnblogs.com/guoyaohua/p/9240336.html</a></p>
<p>鉴于Ngram等模型的不足，2003年，Bengio等人发表了一篇开创性的文章：A neural probabilistic language model[3]。  在这篇文章里，他们总结出了一套用神经网络建立统计语言模型的框架（Neural Network Language Model，以下简称NNLM） ， 奠定了包括word2vec在内后续研究word representation learning的基础。 </p>
<p><strong>NNLM模型的基本思想可以概括如下：</strong></p>
<ol>
<li><strong>假定词表中的每一个word都对应着一个连续的特征向量；</strong></li>
<li><strong>假定一个连续平滑的概率模型，输入一段词向量的序列，可以输出这段序列的联合概率；</strong></li>
<li><strong>同时学习词向量的权重和概率模型里的参数。</strong></li>
</ol>
<p><img src="4.jpg"></p>
<p><img src="5.png"></p>
<p>我们可以将整个模型拆分成两部分加以理解：</p>
<ol>
<li>首先是一个线性的Embedding层。它将输入的N−1个one-hot词向量，通过一个共享的D×V的矩阵C，映射为N−1个分布式的词向量（distributed vector）。其中，V是词典的大小，D是Embedding向量的维度（一个先验参数）。<strong>C矩阵里存储了要学习的word vector。</strong></li>
<li>其次是一个简单的前向反馈神经网络g。它由一个tanh隐层和一个softmax输出层组成。通过将Embedding层输出的N−1个词向量映射为一个长度为V的概率分布向量，从而对词典中的word在输入context下的条件概率做出预估：</li>
</ol>
<p>$$<br>p(w_i|w_1,w_2,…,w_{t-1}) \approx f(w_i, w_{t-1}, …, w_{t-n+1}) = g(w_i, C(w_{t-n+1}), …, C(w_{t-1}))<br>$$</p>
<p><img src="6.jpg"></p>
<h1 id="Word2vec"><a href="#Word2vec" class="headerlink" title="Word2vec"></a>Word2vec</h1><p>原始的NNLM模型的训练其实可以拆分成<strong>两个步骤</strong>：</p>
<ol>
<li>用一个简单模型训练出连续的词向量；</li>
<li>基于词向量的表达，训练一个神经网络模型。<br><strong>而NNLM模型的计算瓶颈主要是在第二步。</strong></li>
</ol>
<p><strong>如果我们只是想得到word的连续特征向量，可以对第二步里的神经网络模型进行简化呢。</strong></p>
<p> <strong>Mikolov 移除前向反馈神经网络中非线性的hidden layer，直接将中间层的Embedding layer与输出层的softmax layer连接；</strong></p>
<p><img src="00.png"></p>
<p><strong>当模型训练完后，最后得到的其实是神经网络的权重，比如现在输入一个 x 的 one-hot encoder: [1,0,0,…,0]，则在输入层到隐含层的权重里，只有对应 1 这个位置的权重被激活，这些权重的个数，跟隐含层节点数是一致的，从而这些权重组成一个向量 Wx 来表示x，而因为每个词语的 one-hot encoder 里面 1 的位置是不同的，所以，这个向量 Wx 就可以用来唯一表示 x。</strong></p>
<p><em><strong>注意：上面这段话说的就是 Word2vec 的精髓！此处依靠矩阵和向量乘法的行列向量空间角度来理解，独热向量只会激活词向量矩阵中的某一列或者某一行。</strong></em></p>
<p>————————————————————————————————————————</p>
<h5 id="困惑已久的解答："><a href="#困惑已久的解答：" class="headerlink" title="困惑已久的解答："></a>困惑已久的解答：</h5><p>Q. gensim 和 google的 word2vec 里面并没有用到onehot encoder，而是初始化的时候直接为每个词随机生成一个N维的向量，并且把这个N维向量作为模型参数学习；所以word2vec结构中不存在文章图中显示的将V维映射到N维的隐藏层。</p>
<p>A. 其实，本质是一样的，加上 one-hot encoder 层，是为了方便理解，因为这里的 N 维随机向量，就可以理解为是 V 维 one-hot encoder 输入层到 N 维隐层的权重，或者说隐层的输出（因为隐层是线性的）。每个 one-hot encoder 里值是 1 的那个位置，对应的 V 个权重被激活，其实就是『从一个V*N的随机词向量矩阵里，抽取某一行』。学习 N 维向量的过程，也就是优化 one-hot encoder 层到隐含层权重的过程</p>
<p><img src="33.png"></p>
<p>————————————————————————————————————————</p>
<h2 id="Skip-gram-和-CBOW-模型"><a href="#Skip-gram-和-CBOW-模型" class="headerlink" title="Skip-gram 和 CBOW 模型"></a>Skip-gram 和 CBOW 模型</h2><p>上面我们提到了语言模型</p>
<ul>
<li>如果是用一个词语作为输入，来预测它周围的上下文，那这个模型叫做『Skip-gram 模型』</li>
<li>而如果是拿一个词语的上下文作为输入，来预测这个词语本身，则是 『CBOW 模型』</li>
</ul>
<p><img src="11.png"></p>
<p><img src="22.jpg"></p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><h6 id="很多NLP任务将word视为atomic-units，-比如n-gram。"><a href="#很多NLP任务将word视为atomic-units，-比如n-gram。" class="headerlink" title="很多NLP任务将word视为atomic units， 比如n-gram。"></a>很多NLP任务将word视为atomic units， 比如n-gram。</h6><h6 id="这种表示方式有一定优势：简洁、健壮性。此外，据观察，简单模型-大量数据的训练效果往往比复杂模型-少量数据的表现更突出。"><a href="#这种表示方式有一定优势：简洁、健壮性。此外，据观察，简单模型-大量数据的训练效果往往比复杂模型-少量数据的表现更突出。" class="headerlink" title="这种表示方式有一定优势：简洁、健壮性。此外，据观察，简单模型+大量数据的训练效果往往比复杂模型+少量数据的表现更突出。"></a>这种表示方式有一定优势：简洁、健壮性。此外，据观察，简单模型+大量数据的训练效果往往比复杂模型+少量数据的表现更突出。</h6><h6 id="这种表示方式也有很多缺点：比如类似于one-hot这样的编码方式用word在vocabulary中的indices作为编码依据，这种方式忽略了word之间的相似性概念。而且，很多任务没有那么多数据。"><a href="#这种表示方式也有很多缺点：比如类似于one-hot这样的编码方式用word在vocabulary中的indices作为编码依据，这种方式忽略了word之间的相似性概念。而且，很多任务没有那么多数据。" class="headerlink" title="这种表示方式也有很多缺点：比如类似于one-hot这样的编码方式用word在vocabulary中的indices作为编码依据，这种方式忽略了word之间的相似性概念。而且，很多任务没有那么多数据。"></a>这种表示方式也有很多缺点：比如类似于one-hot这样的编码方式用word在vocabulary中的indices作为编码依据，这种方式忽略了word之间的相似性概念。而且，很多任务没有那么多数据。</h6><h6 id="随着ML技术兴起，我们可以在更复杂的模型上训练更复杂的数据，word2vec应运而生。"><a href="#随着ML技术兴起，我们可以在更复杂的模型上训练更复杂的数据，word2vec应运而生。" class="headerlink" title="随着ML技术兴起，我们可以在更复杂的模型上训练更复杂的数据，word2vec应运而生。"></a>随着ML技术兴起，我们可以在更复杂的模型上训练更复杂的数据，word2vec应运而生。</h6><h6 id="本文提出的技术，相似的单词在word-vector在vector-space中不仅相近，而且其存在多维度的相似（语义，语法等等维度）"><a href="#本文提出的技术，相似的单词在word-vector在vector-space中不仅相近，而且其存在多维度的相似（语义，语法等等维度）" class="headerlink" title="本文提出的技术，相似的单词在word vector在vector space中不仅相近，而且其存在多维度的相似（语义，语法等等维度）"></a>本文提出的技术，相似的单词在word vector在vector space中不仅相近，而且其存在多维度的相似（语义，语法等等维度）</h6><h6 id="NNLM-a-feedforward-neural-network-with-a-linear-projection-layer-and-a-non-linear-hidden-layer-was-used-to-learn-jointly-the-word-vector-representation-and-a-statistical-language-model"><a href="#NNLM-a-feedforward-neural-network-with-a-linear-projection-layer-and-a-non-linear-hidden-layer-was-used-to-learn-jointly-the-word-vector-representation-and-a-statistical-language-model" class="headerlink" title="NNLM : a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model."></a>NNLM : a feedforward neural network with a linear projection layer and a non-linear hidden layer was used to learn jointly the word vector representation and a statistical language model.</h6><h6 id="在各种大型corpora中使用不同模型训练word-vector-一方面我们获得了word-vector本身，此外，word-vector可以作为输入特征提供给不同NLP任务，能够改善其表现。"><a href="#在各种大型corpora中使用不同模型训练word-vector-一方面我们获得了word-vector本身，此外，word-vector可以作为输入特征提供给不同NLP任务，能够改善其表现。" class="headerlink" title="在各种大型corpora中使用不同模型训练word vector,一方面我们获得了word vector本身，此外，word vector可以作为输入特征提供给不同NLP任务，能够改善其表现。"></a>在各种大型corpora中使用不同模型训练word vector,一方面我们获得了word vector本身，此外，word vector可以作为输入特征提供给不同NLP任务，能够改善其表现。</h6><h2 id="2-Model-Architectures"><a href="#2-Model-Architectures" class="headerlink" title="2 Model Architectures"></a>2 <strong>Model Architectures</strong></h2><h3 id="2-1-Feedforward-Neural-Net-Language-Model-NNLM"><a href="#2-1-Feedforward-Neural-Net-Language-Model-NNLM" class="headerlink" title="2.1 Feedforward Neural Net Language Model (NNLM)"></a>2.1 <strong>Feedforward Neural Net Language Model (NNLM)</strong></h3><h6 id="本模型包括输入层、投影层、隐藏层、输出层。N个word使用-1-V编码，V是vocabulary大小。输入层映射到投影层P，其维度为N-D且共享投影矩阵。（此处具体模型解耦不确定，查一查。）"><a href="#本模型包括输入层、投影层、隐藏层、输出层。N个word使用-1-V编码，V是vocabulary大小。输入层映射到投影层P，其维度为N-D且共享投影矩阵。（此处具体模型解耦不确定，查一查。）" class="headerlink" title="本模型包括输入层、投影层、隐藏层、输出层。N个word使用 1-V编码，V是vocabulary大小。输入层映射到投影层P，其维度为N*D且共享投影矩阵。（此处具体模型解耦不确定，查一查。）"></a>本模型包括输入层、投影层、隐藏层、输出层。N个word使用 1-V编码，V是vocabulary大小。输入层映射到投影层P，其维度为N*D且共享投影矩阵。（此处具体模型解耦不确定，查一查。）</h6><p><img src="7.jpg"></p>
<h3 id="2-2-Recurrent-Neural-Net-Language-Model-RNNLM"><a href="#2-2-Recurrent-Neural-Net-Language-Model-RNNLM" class="headerlink" title="2.2  Recurrent Neural Net Language Model (RNNLM)"></a>2.2  <strong>Recurrent Neural Net Language Model (RNNLM)</strong></h3><h6 id="RNNLM可以有效学习深层的复杂模式（浅层模型学不到的）。"><a href="#RNNLM可以有效学习深层的复杂模式（浅层模型学不到的）。" class="headerlink" title="RNNLM可以有效学习深层的复杂模式（浅层模型学不到的）。"></a>RNNLM可以有效学习深层的复杂模式（浅层模型学不到的）。</h6><h6 id="RNN-将隐藏层指向自己，这使得其具备短时记忆，隐层节点可以记住过去的信息，并且可以通过当前的输入和隐层信息来更新。"><a href="#RNN-将隐藏层指向自己，这使得其具备短时记忆，隐层节点可以记住过去的信息，并且可以通过当前的输入和隐层信息来更新。" class="headerlink" title="RNN 将隐藏层指向自己，这使得其具备短时记忆，隐层节点可以记住过去的信息，并且可以通过当前的输入和隐层信息来更新。"></a>RNN 将隐藏层指向自己，这使得其具备短时记忆，隐层节点可以记住过去的信息，并且可以通过当前的输入和隐层信息来更新。</h6><h2 id="-1"><a href="#-1" class="headerlink" title=""></a></h2><h2 id="3-New-Log-linear-Models"><a href="#3-New-Log-linear-Models" class="headerlink" title="3  New Log-linear Models"></a>3  <strong>New Log-linear Models</strong></h2><p><img src="0.jpg"></p>
<h3 id="3-1Continuous-Bag-of-Words-Model"><a href="#3-1Continuous-Bag-of-Words-Model" class="headerlink" title="3.1Continuous Bag-of-Words Model"></a>3.1<strong>Continuous Bag-of-Words Model</strong></h3><h6 id="依据NNLM，去掉了非线性隐藏层且投影层共享（不仅仅共享投影矩阵），投影结果求和再求均值。"><a href="#依据NNLM，去掉了非线性隐藏层且投影层共享（不仅仅共享投影矩阵），投影结果求和再求均值。" class="headerlink" title="依据NNLM，去掉了非线性隐藏层且投影层共享（不仅仅共享投影矩阵），投影结果求和再求均值。"></a>依据NNLM，去掉了非线性隐藏层且投影层共享（不仅仅共享投影矩阵），投影结果求和再求均值。</h6><h6 id="此模型称为bag-of-words-model，因为以上求均值操作导致order-of-words-不影响投影结果。"><a href="#此模型称为bag-of-words-model，因为以上求均值操作导致order-of-words-不影响投影结果。" class="headerlink" title="此模型称为bag-of-words model，因为以上求均值操作导致order of words 不影响投影结果。"></a>此模型称为bag-of-words model，因为以上求均值操作导致order of words 不影响投影结果。</h6><h3 id="-2"><a href="#-2" class="headerlink" title=""></a></h3><h3 id="3-2-Continuous-Skip-gram-Model"><a href="#3-2-Continuous-Skip-gram-Model" class="headerlink" title="3.2 Continuous Skip-gram Model"></a><strong>3.2 Continuous Skip-gram Model</strong></h3><h6 id="与CBOW类似，只是，CBOW是用上下文预测当前word，而Skip-gram是用当前word来预测上下文。"><a href="#与CBOW类似，只是，CBOW是用上下文预测当前word，而Skip-gram是用当前word来预测上下文。" class="headerlink" title="与CBOW类似，只是，CBOW是用上下文预测当前word，而Skip-gram是用当前word来预测上下文。"></a>与CBOW类似，只是，CBOW是用上下文预测当前word，而Skip-gram是用当前word来预测上下文。</h6><h2 id="4-Results"><a href="#4-Results" class="headerlink" title="4 Results"></a>4 Results</h2><h6 id="简单相似：France与Italy之间的vector相似。"><a href="#简单相似：France与Italy之间的vector相似。" class="headerlink" title="简单相似：France与Italy之间的vector相似。"></a>简单相似：France与Italy之间的vector相似。</h6><h6 id="更复杂的相似：word-big-is-similar-to-bigger-in-the-same-sense-that-small-is-similar-to-smaller"><a href="#更复杂的相似：word-big-is-similar-to-bigger-in-the-same-sense-that-small-is-similar-to-smaller" class="headerlink" title="更复杂的相似：word big is similar to bigger in the same sense that small is similar to smaller."></a>更复杂的相似：word <em>big</em> is similar to <em>bigger</em> in the same sense that <em>small</em> is similar to <em>smaller</em>.</h6><h6 id="Word-vectors-with-such-semantic-relationships-could-be-used-to-improve-many-existing-NLP-applications-such-as-machine-translation-information-retrieval-and-question-answering-systems-and-may-enable-other-future-applications-yet-to-be-invented"><a href="#Word-vectors-with-such-semantic-relationships-could-be-used-to-improve-many-existing-NLP-applications-such-as-machine-translation-information-retrieval-and-question-answering-systems-and-may-enable-other-future-applications-yet-to-be-invented" class="headerlink" title="Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented."></a>Word vectors with such semantic relationships could be used to improve many existing NLP applications, such as machine translation, information retrieval and question answering systems, and may enable other future applications yet to be invented.</h6><h6 id="易知，使用更多的数据和更高维的word-vector可以提高accuracy，但训练时间也更长。"><a href="#易知，使用更多的数据和更高维的word-vector可以提高accuracy，但训练时间也更长。" class="headerlink" title="易知，使用更多的数据和更高维的word vector可以提高accuracy，但训练时间也更长。"></a>易知，使用更多的数据和更高维的word vector可以提高accuracy，但训练时间也更长。</h6>
                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">HZL</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://huizhilei.github.io/2021/11/06/word2vec%E8%AE%BA%E6%96%87/">https://huizhilei.github.io/2021/11/06/word2vec%E8%AE%BA%E6%96%87/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">HZL</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/Efficient-Estimation-of-Word-Representations-in-Vector-Space/">
                                    <span class="chip bg-color">Efficient Estimation of Word Representations in Vector Space</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2021/11/06/paragraph-vector%E8%AE%BA%E6%96%87/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/9.jpg" class="responsive-img" alt="paragraph-vector论文">
                        
                        <span class="card-title">paragraph-vector论文</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2021-11-06
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            HZL
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Distributed-Representations-of-Sentences-and-Documents/">
                        <span class="chip bg-color">Distributed Representations of Sentences and Documents</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                本篇&nbsp;<i class="far fa-dot-circle"></i>
            </div>
            <div class="card">
                <a href="/2021/11/06/word2vec%E8%AE%BA%E6%96%87/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/17.jpg" class="responsive-img" alt="word2vec论文">
                        
                        <span class="card-title">word2vec论文</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2021-11-06
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-user fa-fw"></i>
                            HZL
                            
                        </span>
                    </div>
                </div>

                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Efficient-Estimation-of-Word-Representations-in-Vector-Space/">
                        <span class="chip bg-color">Efficient Estimation of Word Representations in Vector Space</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>




    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2019-2022</span>
            
            <a href="/about" target="_blank">HZL</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/huizhilei" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:897947313@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=897947313" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 897947313" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    

    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
